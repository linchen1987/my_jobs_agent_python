import os
import json
import logging
from datetime import datetime
from dotenv import load_dotenv
from typing import List, Dict, Any, Tuple

from tools.fetch_and_parse_all import fetch_and_parse_all
# from tools.config import source_list
from tools.llm_doubao import DoubaoChat
from tools.analyze_data import analyze_job_with_llm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

source_list: list[str] = [
    "https://svc.eleduck.com/api/v1/posts?page=1"
]
OFFSET = 0
LIMIT = 0

# é…ç½®æ—¥å¿—
log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)


def save_notifications(new_qualified_jobs: List[Dict[str, Any]]) -> None:
    """
    ä¿å­˜æ–°çš„ç¬¦åˆæ¡ä»¶çš„èŒä½åˆ°é€šçŸ¥æ–‡ä»¶
    
    Args:
        new_qualified_jobs: æ–°çš„ç¬¦åˆæ¡ä»¶çš„èŒä½åˆ—è¡¨
    """
    if not new_qualified_jobs:
        return
    
    notifications_path = ".data/jobs_notifications.md"
    
    # ç”Ÿæˆæ–°é€šçŸ¥çš„Markdownå†…å®¹
    new_notification_content = create_notification_markdown(new_qualified_jobs)
    
    # åŠ è½½ç°æœ‰é€šçŸ¥å†…å®¹
    existing_content = ""
    if os.path.exists(notifications_path):
        try:
            with open(notifications_path, "r", encoding="utf-8") as f:
                existing_content = f.read()
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç°æœ‰é€šçŸ¥å¤±è´¥: {e}")
            existing_content = ""
    
    # åˆå¹¶å†…å®¹ï¼šæ–°é€šçŸ¥åœ¨å‰é¢
    if existing_content:
        all_content = new_notification_content + "\n---\n\n" + existing_content
    else:
        all_content = new_notification_content
    
    # ä¿å­˜é€šçŸ¥æ–‡ä»¶
    try:
        with open(notifications_path, "w", encoding="utf-8") as f:
            f.write(all_content)
        print(f"\nğŸ“¢ å‘ç° {len(new_qualified_jobs)} ä¸ªæ–°èŒä½ï¼Œå·²ä¿å­˜åˆ° {notifications_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜é€šçŸ¥å¤±è´¥: {e}")


def create_notification_markdown(qualified_jobs: List[Dict[str, Any]]) -> str:
    """
    åˆ›å»ºæ–°èŒä½é€šçŸ¥çš„Markdownå†…å®¹
    
    Args:
        qualified_jobs: ç¬¦åˆæ¡ä»¶çš„èŒä½åˆ—è¡¨
        
    Returns:
        str: æ ¼å¼åŒ–çš„Markdowné€šçŸ¥å†…å®¹
    """
    if not qualified_jobs:
        return ""
    
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åˆ›å»ºé€šçŸ¥å†…å®¹
    markdown = f"# æ–°èŒä½é€šçŸ¥ - {current_time}\n\n"
    markdown += f"å‘ç° {len(qualified_jobs)} ä¸ªæ–°çš„ç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯ï¼š\n\n"
    markdown += "| URL | ID | å…¬å¸ä»‹ç» | å…¬å¸ç½‘ç«™ | æŠ€èƒ½è¦æ±‚ | è–ªèµ„å¾…é‡ |\n"
    markdown += "|-----|----|---------|---------|---------|---------|\n"
    
    # æ·»åŠ æ¯è¡Œæ•°æ®
    for job in qualified_jobs:
        original_data = job.get("original_data", {})
        list_metadata = original_data.get("list_metadata", {})
        llm_analysis = job.get("llm_analysis", {})
        extracted_info = llm_analysis.get("extracted_info", {})
        
        # è·å–å„å­—æ®µä¿¡æ¯
        url = list_metadata.get("url", "")
        job_id = list_metadata.get("id", "")
        company_intro = extracted_info.get("company_introduction", "æœªæåŠ")
        company_website = extracted_info.get("company_website", "æœªæåŠ")
        skill_req = extracted_info.get("skill_requirements", "æœªæåŠ")
        salary = extracted_info.get("salary_benefits", "æœªæåŠ")
        
        # æ¸…ç†æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦å’Œç®¡é“ç¬¦
        def clean_text(text):
            if not text or text == "æœªæåŠ":
                return "æœªæåŠ"
            return str(text).replace('\n', ' ').replace('|', 'ï½œ').strip()
        
        company_intro = clean_text(company_intro)
        company_website = clean_text(company_website)
        skill_req = clean_text(skill_req)
        salary = clean_text(salary)
        
        # é™åˆ¶é•¿åº¦é¿å…è¡¨æ ¼è¿‡å®½
        def truncate_text(text, max_length=100):
            if len(text) > max_length:
                return text[:max_length] + "..."
            return text
        
        company_intro = truncate_text(company_intro)
        skill_req = truncate_text(skill_req)
        
        markdown += f"| [{job_id}]({url}) | {job_id} | {company_intro} | {company_website} | {skill_req} | {salary} |\n"
    
    return markdown


def create_markdown_table(qualified_jobs: List[Dict[str, Any]]) -> str:
    """
    åˆ›å»ºåŒ…å«åˆæ ¼èŒä½çš„Markdownè¡¨æ ¼
    
    Args:
        qualified_jobs: é€šè¿‡ç­›é€‰çš„èŒä½åˆ—è¡¨
        
    Returns:
        str: æ ¼å¼åŒ–çš„Markdownè¡¨æ ¼
    """
    if not qualified_jobs:
        return "## åˆ†æç»“æœ\n\næš‚æ— ç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯ã€‚\n"
    
    # åˆ›å»ºè¡¨æ ¼å¤´
    markdown = "## æ‹›è˜ä¿¡æ¯åˆ†æç»“æœ\n\n"
    markdown += f"å…±æ‰¾åˆ° {len(qualified_jobs)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯ï¼š\n\n"
    markdown += "| URL | ID | å…¬å¸ä»‹ç» | å…¬å¸ç½‘ç«™ | æŠ€èƒ½è¦æ±‚ | è–ªèµ„å¾…é‡ |\n"
    markdown += "|-----|----|---------|---------|---------|---------|\n"
    
    # æ·»åŠ æ¯è¡Œæ•°æ®
    for job in qualified_jobs:
        original_data = job.get("original_data", {})
        list_metadata = original_data.get("list_metadata", {})
        llm_analysis = job.get("llm_analysis", {})
        extracted_info = llm_analysis.get("extracted_info", {})
        
        # è·å–å„å­—æ®µä¿¡æ¯
        url = list_metadata.get("url", "")
        job_id = list_metadata.get("id", "")
        company_intro = extracted_info.get("company_introduction", "æœªæåŠ")
        company_website = extracted_info.get("company_website", "æœªæåŠ")
        skill_req = extracted_info.get("skill_requirements", "æœªæåŠ")
        salary = extracted_info.get("salary_benefits", "æœªæåŠ")
        
        # æ¸…ç†æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦å’Œç®¡é“ç¬¦
        def clean_text(text):
            if not text or text == "æœªæåŠ":
                return "æœªæåŠ"
            return str(text).replace('\n', ' ').replace('|', 'ï½œ').strip()
        
        company_intro = clean_text(company_intro)
        company_website = clean_text(company_website)
        skill_req = clean_text(skill_req)
        salary = clean_text(salary)
        
        # é™åˆ¶é•¿åº¦é¿å…è¡¨æ ¼è¿‡å®½
        def truncate_text(text, max_length=100):
            if len(text) > max_length:
                return text[:max_length] + "..."
            return text
        
        company_intro = truncate_text(company_intro)
        skill_req = truncate_text(skill_req)
        
        markdown += f"| [{job_id}]({url}) | {job_id} | {company_intro} | {company_website} | {skill_req} | {salary} |\n"
    
    return markdown


def process_data() -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    æ•°æ®å¤„ç†å‡½æ•°ï¼šè´Ÿè´£æŠ“å–æ•°æ®ã€åˆ†ææ‹›è˜ä¿¡æ¯
    
    Returns:
        Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]: 
        (new_analyzed_records, new_qualified_jobs)
    """
    print("=== å¼€å§‹æ•°æ®å¤„ç†é˜¶æ®µ ===\n")
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    print("ğŸš€ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
    llm_client = DoubaoChat()
    
    # è·å–æ‰€æœ‰æ•°æ®
    print("\nğŸ“¥ å¼€å§‹æŠ“å–æ•°æ®...")
    all_jobs_data = fetch_and_parse_all(source_list, offset=OFFSET, limit=LIMIT)
    
    if not all_jobs_data:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
        return [], []
    
    print(f"\nğŸ“Š è·å–åˆ° {len(all_jobs_data)} ä¸ªæ‹›è˜ä¿¡æ¯ï¼Œå¼€å§‹åˆ†æ...")
    
    # ç¡®ä¿.dataç›®å½•å­˜åœ¨
    os.makedirs(".data", exist_ok=True)
    
    # åŠ è½½å·²åˆ†æè®°å½•è¡¨
    analyzed_jobs = []
    analyzed_json_path = ".data/analyzed_jobs.json"
    if os.path.exists(analyzed_json_path):
        try:
            with open(analyzed_json_path, "r", encoding="utf-8") as f:
                analyzed_jobs = json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å·²åˆ†æè®°å½•å¤±è´¥: {e}")
            analyzed_jobs = []
    
    # åˆ›å»ºå·²åˆ†ææ•°æ®çš„ç´¢å¼•ï¼ˆåŸºäºIDï¼‰
    analyzed_ids = set()
    for record in analyzed_jobs:
        job_id = record.get("id", "")
        if job_id:
            analyzed_ids.add(job_id)
    
    # è¿‡æ»¤æ‰å·²åˆ†æè¿‡çš„æ•°æ®
    unanalyzed_jobs_data = []
    skipped_count = 0
    
    for job_data in all_jobs_data:
        list_metadata = job_data.get("list_metadata", {})
        job_id = list_metadata.get("id", "")
        
        if job_id in analyzed_ids:
            skipped_count += 1
        else:
            unanalyzed_jobs_data.append(job_data)
    
    print(f"ğŸ“Š è¿‡æ»¤ç»“æœ: æ€»å…± {len(all_jobs_data)} ä¸ªæ‹›è˜ä¿¡æ¯ï¼Œè·³è¿‡ {skipped_count} ä¸ªå·²åˆ†æçš„ï¼Œéœ€è¦åˆ†æ {len(unanalyzed_jobs_data)} ä¸ªæ–°çš„")
    
    # åˆ†æè¿‡æ»¤åçš„æ‹›è˜ä¿¡æ¯
    new_qualified_jobs = []
    new_analyzed_records = []
    current_time = datetime.now().isoformat()
    
    for i, job_data in enumerate(unanalyzed_jobs_data, 1):
        list_metadata = job_data.get("list_metadata", {})
        job_id = list_metadata.get("id", "")
        job_url = list_metadata.get("url", "")
        
        print(f"\n[{i}/{len(unanalyzed_jobs_data)}] åˆ†æä¸­: {job_id}")
        
        # ä½¿ç”¨LLMåˆ†æ
        analysis_result = analyze_job_with_llm(llm_client, job_data)
        
        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ¡ä»¶
        llm_analysis = analysis_result.get("llm_analysis", {})
        is_qualified = llm_analysis.get("is_qualified", False)
        analysis_detail = llm_analysis.get("analysis", {})
        qualification_reason = analysis_detail.get("reasoning", "")
        
        # è®°å½•åˆ°å·²åˆ†æè¡¨ä¸­
        analyzed_record = {
            "id": job_id,
            "url": job_url,
            "is_qualified": is_qualified,
            "createdAt": current_time,
            "reason": qualification_reason
        }
        new_analyzed_records.append(analyzed_record)
        
        if is_qualified:
            new_qualified_jobs.append(analysis_result)
            print("âœ… ç¬¦åˆæ¡ä»¶")
        else:
            print("âŒ ä¸ç¬¦åˆæ¡ä»¶")
    
    print(f"\nğŸ“ˆ æ•°æ®å¤„ç†å®Œæˆï¼è·³è¿‡ {skipped_count} ä¸ªå·²åˆ†æé¡¹ï¼Œæ–°å¢ {len(new_qualified_jobs)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯")
    
    return new_analyzed_records, new_qualified_jobs


def handle_results(new_analyzed_records: List[Dict[str, Any]], 
                  new_qualified_jobs: List[Dict[str, Any]]) -> None:
    """
    åç»­åŠ¨ä½œå‡½æ•°ï¼šè´Ÿè´£å­˜å‚¨ã€é€šçŸ¥ç­‰æ“ä½œ
    
    Args:
        new_analyzed_records: æ–°çš„å·²åˆ†æè®°å½•åˆ—è¡¨
        new_qualified_jobs: æ–°çš„ç¬¦åˆæ¡ä»¶çš„èŒä½åˆ—è¡¨
    """
    print("\n=== å¼€å§‹åç»­åŠ¨ä½œé˜¶æ®µ ===\n")
    
    # åŠ è½½ç°æœ‰æ•°æ®
    analyzed_json_path = ".data/analyzed_jobs.json"
    jobs_json_path = ".data/jobs.json"
    
    # åŠ è½½ç°æœ‰çš„å·²åˆ†æè®°å½•
    existing_analyzed_jobs = []
    if os.path.exists(analyzed_json_path):
        try:
            with open(analyzed_json_path, "r", encoding="utf-8") as f:
                existing_analyzed_jobs = json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç°æœ‰å·²åˆ†æè®°å½•å¤±è´¥: {e}")
            existing_analyzed_jobs = []
    
    # åŠ è½½ç°æœ‰çš„ç¬¦åˆæ¡ä»¶çš„ç»“æœ
    existing_qualified_results = []
    if os.path.exists(jobs_json_path):
        try:
            with open(jobs_json_path, "r", encoding="utf-8") as f:
                existing_qualified_results = json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç°æœ‰ç¬¦åˆæ¡ä»¶ç»“æœå¤±è´¥: {e}")
            existing_qualified_results = []
    
    # æ›´æ–°å·²åˆ†æè®°å½•è¡¨ï¼šæ–°è®°å½•åœ¨å‰é¢
    all_analyzed_jobs = new_analyzed_records + existing_analyzed_jobs
    
    # ä¿å­˜å·²åˆ†æè®°å½•è¡¨
    print(f"ğŸ’¾ ä¿å­˜å·²åˆ†æè®°å½•è¡¨...ï¼ˆæ€»å…± {len(all_analyzed_jobs)} ä¸ªå·²åˆ†æè®°å½•ï¼‰")
    with open(analyzed_json_path, "w", encoding="utf-8") as f:
        json.dump(all_analyzed_jobs, f, ensure_ascii=False, indent=2)
    
    # åˆå¹¶ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼šæ–°æ•°æ®åœ¨å‰é¢
    all_qualified_jobs = new_qualified_jobs + existing_qualified_results
    
    # ä¿å­˜ç¬¦åˆæ¡ä»¶çš„JSONæ–‡ä»¶
    print(f"ğŸ’¾ ä¿å­˜ç¬¦åˆæ¡ä»¶çš„æ•°æ®...ï¼ˆæ€»å…± {len(all_qualified_jobs)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯ï¼‰")
    with open(jobs_json_path, "w", encoding="utf-8") as f:
        json.dump(all_qualified_jobs, f, ensure_ascii=False, indent=2)
    
    # ç”ŸæˆMarkdown
    print("ğŸ“ ç”ŸæˆMarkdownæŠ¥å‘Š...")
    markdown_content = create_markdown_table(all_qualified_jobs)
    
    # å†™å…¥æ–‡ä»¶
    with open(".data/jobs.md", "w", encoding="utf-8") as f:
        f.write(markdown_content)
    
    print("âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° jobs.json å’Œ jobs.md")
    
    # ä¿å­˜æ–°çš„ç¬¦åˆæ¡ä»¶çš„èŒä½åˆ°é€šçŸ¥æ–‡ä»¶
    if new_qualified_jobs:
        save_notifications(new_qualified_jobs)


def main():
    """ä¸»å‡½æ•°ï¼šåè°ƒæ•°æ®å¤„ç†å’Œåç»­åŠ¨ä½œ"""
    print("=== å¼€å§‹æ‹›è˜ä¿¡æ¯åˆ†ææµç¨‹ ===\n")
    
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å¤„ç†
        new_analyzed_records, new_qualified_jobs = process_data()
        
        # ç¬¬äºŒé˜¶æ®µï¼šåç»­åŠ¨ä½œ
        handle_results(new_analyzed_records, new_qualified_jobs)
        
        print(f"\nğŸ‰ æµç¨‹å®Œæˆï¼æ–°å¢ {len(new_qualified_jobs)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()